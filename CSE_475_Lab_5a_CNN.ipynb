{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9797449,
          "sourceType": "datasetVersion",
          "datasetId": 6004320
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neutr0nslayer/CSE475/blob/main/CSE_475_Lab_5a_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pumpkin Leaf Disease Detection using CNNs and XAI using PyTorch\n",
        "\n",
        "\n",
        "\n",
        "**This dataset and analysis has been published in COMPAS 2024 Conference**\n",
        "\n",
        "\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "1. Custom CNN model from scratch for leaf disease detection.\n",
        "\n",
        "2. Transfer learning models (VGG, MobileNetV2, EfficientNet, DenseNet121, InceptionNet).\n",
        "\n",
        "3. Training with dataset augmentation and early stopping.\n",
        "\n",
        "4. Visualization of training and validation loss curves.\n",
        "\n",
        "5. Model evaluation with precision, recall, and F1-score.\n",
        "\n",
        "6. XAI techniques: Grad-CAM, Grad-CAM++, Eigen-CAM, and LIME for model interpretation."
      ],
      "metadata": {
        "id": "JUxXWU9jZgYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Models"
      ],
      "metadata": {
        "id": "v5682PzjaD4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "3l1wpcJZaCFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
        "\n",
        "\n",
        "!pip install lime"
      ],
      "metadata": {
        "id": "K5b5g5qVc6vI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T19:50:13.240959Z",
          "iopub.execute_input": "2024-11-03T19:50:13.241819Z",
          "iopub.status.idle": "2024-11-03T19:50:55.082303Z",
          "shell.execute_reply.started": "2024-11-03T19:50:13.241781Z",
          "shell.execute_reply": "2024-11-03T19:50:55.081330Z"
        },
        "outputId": "7807afc5-4eea-40d2-f8b8-f27bc0fef7a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
            "  Cloning https://github.com/jacobgil/pytorch-grad-cam.git to /tmp/pip-req-build-bbs7i4td\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jacobgil/pytorch-grad-cam.git /tmp/pip-req-build-bbs7i4td\n",
            "  Resolved https://github.com/jacobgil/pytorch-grad-cam.git to commit 781dbc0d16ffa95b6d18b96b7b829840a82d93d1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (11.1.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (0.21.0+cu124)\n",
            "Collecting ttach (from grad-cam==1.5.5)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.1->grad-cam==1.5.5)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam==1.5.5) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam==1.5.5) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam==1.5.5) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam==1.5.5) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam==1.5.5) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam==1.5.5) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=43670 sha256=bc40618d16edcb600bd732783e1955bb33bb9d711976ef8351663374369775dc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4gl29ynf/wheels/a0/4d/c8/0502c44e32030c99ffab1b98075308a1ef9829c1835537afc6\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, grad-cam\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed grad-cam-1.5.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ttach-0.0.3\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=1fb590de2b4134a8694e1d9c6db70008d453e4366d776485bdf919b647fb359c\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, EigenCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from lime import lime_image\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ],
      "metadata": {
        "id": "qwK3W1HLZwSo",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T19:50:55.084398Z",
          "iopub.execute_input": "2024-11-03T19:50:55.084820Z",
          "iopub.status.idle": "2024-11-03T19:51:01.207704Z",
          "shell.execute_reply.started": "2024-11-03T19:50:55.084774Z",
          "shell.execute_reply": "2024-11-03T19:51:01.206879Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare the Dataset\n",
        "\n",
        "Assuming the dataset is structured in folders for each class, we'll set up PyTorch data loaders for training, validation, and testing. Ensure the dataset is unzipped and organized properly."
      ],
      "metadata": {
        "id": "0GTeCYHWaNK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/rifat963/pumpkin/versions/2"
      ],
      "metadata": {
        "id": "Cyfy7nOuZrWi",
        "outputId": "e1f9dc87-3e9f-4b8f-d8b6-9a0d19a1b425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/root/.cache/kagglehub/datasets/rifat963/pumpkin/versions/2': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub, shutil, os\n",
        "\n",
        "# 1. Download the dataset\n",
        "src_root = kagglehub.dataset_download(\"rifat963/pumpkin\")\n",
        "\n",
        "# 2. Define where you want it in Colab\n",
        "dst_root = \"/content/pumpkin\"\n",
        "os.makedirs(dst_root, exist_ok=True)\n",
        "\n",
        "# 3. Copy every top-level folder (Augmented, Test, Train, Valid) with their subfolders\n",
        "for folder in os.listdir(src_root):\n",
        "    src = os.path.join(src_root, folder)\n",
        "    dst = os.path.join(dst_root, folder)\n",
        "    if os.path.isdir(src):\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "\n",
        "print(f\"Dataset successfully copied to {dst_root}\")"
      ],
      "metadata": {
        "id": "21Vj6GTxughW",
        "outputId": "66ca0434-d9b4-4ebe-d605-c2fa2dbe69d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rifat963/pumpkin?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675M/675M [00:07<00:00, 93.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully copied to /content/pumpkin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Define your transforms\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # assuming RGB images—three channels\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# 2. Point to your Augmented folder\n",
        "data_root = \"/content/pumpkin/Augmented/Augmented\"\n",
        "\n",
        "# 3. Create one ImageFolder per split\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_root, \"Train\"),\n",
        "    transform=transform_train\n",
        ")\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_root, \"Valid\"),\n",
        "    transform=transform_test\n",
        ")\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    os.path.join(data_root, \"Test\"),\n",
        "    transform=transform_test\n",
        ")\n",
        "\n",
        "# 4. Wrap in DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Splits: train={len(train_dataset)}  valid={len(val_dataset)}  test={len(test_dataset)}\")\n",
        "class_names = train_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "print(class_names)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "u4pW29Bxythg",
        "outputId": "fea3c105-f057-4717-bf39-1f5db4ccc8b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/pumpkin/Augmented/Augmented/Train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-894c289b9fa7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 3. Create one ImageFolder per split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m train_dataset = datasets.ImageFolder(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/pumpkin/Augmented/Augmented/Train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G8PXc4hf7le2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Example Images for Each Class"
      ],
      "metadata": {
        "id": "l9tsuJZpdly4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Get class names\n",
        "classes = train_dataset.classes  # e.g. ['Bacterial Leaf Spot', 'Downy Mildew', …]\n",
        "\n",
        "# 2. Pull out one sample per class\n",
        "examples = {}\n",
        "for img, label in train_dataset:\n",
        "    if label not in examples:\n",
        "        examples[label] = img\n",
        "    if len(examples) == len(classes):\n",
        "        break\n",
        "\n",
        "# 3. Plot them\n",
        "fig, axes = plt.subplots(1, len(classes), figsize=(3*len(classes), 3))\n",
        "for idx, (label, img_tensor) in enumerate(examples.items()):\n",
        "    ax = axes[idx]\n",
        "    # un-normalize\n",
        "    img = img_tensor.numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std  = np.array([0.5, 0.5, 0.5])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(classes[label], fontsize=9)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZUSAhE2zzXUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a Custom CNN Model"
      ],
      "metadata": {
        "id": "P50_wXIpdwge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, len(class_names))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 64 * 56 * 56)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ymzZzgvJdnoQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T19:52:26.999047Z",
          "iopub.execute_input": "2024-11-03T19:52:26.999739Z",
          "iopub.status.idle": "2024-11-03T19:52:27.008615Z",
          "shell.execute_reply.started": "2024-11-03T19:52:26.999696Z",
          "shell.execute_reply": "2024-11-03T19:52:27.007565Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning Models\n",
        "\n",
        "Using pretrained models from PyTorch."
      ],
      "metadata": {
        "id": "fMOfMMKVd0yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "def get_transfer_model(model_name, num_classes):\n",
        "    if model_name == \"vgg16\":\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"mobilenet_v2\":\n",
        "        model = models.mobilenet_v2(pretrained=True)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"efficientnet_b0\":\n",
        "        model = models.efficientnet_b0(pretrained=True)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"densenet121\":\n",
        "        model = models.densenet121(pretrained=True)\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"inception_v3\":\n",
        "        model = models.inception_v3(pretrained=True)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    elif model_name == \"resnet50\":\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify the final fully connected layer\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Model '{model_name}' not supported. Choose from vgg16, mobilenet_v2, efficientnet_b0, densenet121, inception_v3, resnet50.\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "si8v45vpd2J_",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T19:52:53.711811Z",
          "iopub.execute_input": "2024-11-03T19:52:53.712193Z",
          "iopub.status.idle": "2024-11-03T19:52:53.722063Z",
          "shell.execute_reply.started": "2024-11-03T19:52:53.712157Z",
          "shell.execute_reply": "2024-11-03T19:52:53.721082Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Early Stopping\n",
        "\n",
        "Include early stopping logic."
      ],
      "metadata": {
        "id": "tgUHi8q4d7NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "    def __init__(self, patience=5):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = np.inf\n",
        "\n",
        "    def check_early_stop(self, val_loss):\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "eeBEQb_td85f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T19:52:57.853389Z",
          "iopub.execute_input": "2024-11-03T19:52:57.854208Z",
          "iopub.status.idle": "2024-11-03T19:52:57.860410Z",
          "shell.execute_reply.started": "2024-11-03T19:52:57.854166Z",
          "shell.execute_reply": "2024-11-03T19:52:57.859304Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model and Plot Loss Curves"
      ],
      "metadata": {
        "id": "BTH8lREld_Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training using Custom Model (Slow Without AMP)"
      ],
      "metadata": {
        "id": "LhGn7ljDOO7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs and initialize variables\n",
        "\n",
        "num_epochs = 10  # Define the number of training epochs\n",
        "\n",
        "model = CustomCNN()  # Initialize the model; can replace with a transfer model like get_transfer_model()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer with learning rate\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5)  # Initialize early stopping with patience\n",
        "\n",
        "train_losses, val_losses = [], []  # Lists to store training and validation losses per epoch\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    # Training phase\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    train_loss = 0  # Initialize cumulative training loss for the epoch\n",
        "\n",
        "    # Loop over training data with tqdm progress bar\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()  # Backward pass (calculate gradients)\n",
        "        optimizer.step()  # Update weights\n",
        "        train_loss += loss.item()  # Accumulate the training loss\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0  # Initialize cumulative validation loss for the epoch\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "\n",
        "        # Loop over validation data with tqdm progress bar\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            val_loss += loss.item()  # Accumulate the validation loss\n",
        "\n",
        "    # Calculate average losses and append to lists\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping.check_early_stop(avg_val_loss):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "Ts-5z0_CeB2n",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T19:53:29.421941Z",
          "iopub.execute_input": "2024-11-03T19:53:29.422634Z",
          "iopub.status.idle": "2024-11-03T20:10:29.915364Z",
          "shell.execute_reply.started": "2024-11-03T19:53:29.422594Z",
          "shell.execute_reply": "2024-11-03T20:10:29.914335Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a path for saving the model\n",
        "model_save_path = \"custom_cnn_model.pth\"  # You can specify a different path or filename\n",
        "\n",
        "# Save the model after training completes or early stopping is triggered\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:11:25.499580Z",
          "iopub.execute_input": "2024-11-03T20:11:25.500335Z",
          "iopub.status.idle": "2024-11-03T20:11:25.621330Z",
          "shell.execute_reply.started": "2024-11-03T20:11:25.500293Z",
          "shell.execute_reply": "2024-11-03T20:11:25.620268Z"
        },
        "id": "ni6EWtrVOO8A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting loss curves\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:11:30.295459Z",
          "iopub.execute_input": "2024-11-03T20:11:30.296156Z",
          "iopub.status.idle": "2024-11-03T20:11:30.533302Z",
          "shell.execute_reply.started": "2024-11-03T20:11:30.296107Z",
          "shell.execute_reply": "2024-11-03T20:11:30.532363Z"
        },
        "id": "tVh-HsgjOO8B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using AMP with Custom Model\n",
        "\n",
        "\n",
        "\n",
        "To improve training speed, we can utilize Automatic Mixed Precision (AMP) provided by torch.cuda.amp. AMP enables faster training by using lower precision (float16) for parts of the computation while maintaining accuracy with some operations in higher precision (float32)."
      ],
      "metadata": {
        "id": "xkYgMEwbgmPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "from torch.cuda.amp import autocast, GradScaler  # Import AMP utilities\n",
        "\n",
        "# Set number of epochs and initialize variables\n",
        "\n",
        "num_epochs = 10  # Define the number of training epochs\n",
        "\n",
        "model = CustomCNN().to('cuda')  # Move model to GPU\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer with learning rate\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5)  # Initialize early stopping with patience\n",
        "\n",
        "train_losses, val_losses = [], []  # Lists to store training and validation losses per epoch\n",
        "\n",
        "# Initialize GradScaler for AMP\n",
        "scaler = GradScaler()\n",
        "# Loop over epochs\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    # Training phase\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0  # Initialize cumulative training loss for the epoch\n",
        "    # Loop over training data with tqdm progress bar\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        with autocast():  # Use AMP for mixed-precision calculations\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "        # Scale loss to avoid underflow for float16\n",
        "        scaler.scale(loss).backward()  # Backward pass with scaled loss\n",
        "        scaler.step(optimizer)  # Optimizer step\n",
        "        scaler.update()  # Update the scaler for next iteration\n",
        "        train_loss += loss.item()  # Accumulate the training loss\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0  # Initialize cumulative validation loss for the epoch\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        # Loop over validation data with tqdm progress bar\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            images, labels = images.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "            with autocast():  # Use AMP for mixed-precision calculations\n",
        "                outputs = model(images)  # Forward pass\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "            val_loss += loss.item()  # Accumulate the validation loss\n",
        "\n",
        "    # Calculate average losses and append to lists\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping.check_early_stop(avg_val_loss):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "HY0q9STMgsI3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:12:51.490875Z",
          "iopub.execute_input": "2024-11-03T20:12:51.491632Z",
          "iopub.status.idle": "2024-11-03T20:15:26.496631Z",
          "shell.execute_reply.started": "2024-11-03T20:12:51.491585Z",
          "shell.execute_reply": "2024-11-03T20:15:26.495629Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning Example using ResNet50"
      ],
      "metadata": {
        "id": "fFXz2BbRiJM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "from torch.cuda.amp import autocast, GradScaler  # Import AMP utilities\n",
        "\n",
        "# Set number of epochs and initialize variables\n",
        "num_epochs = 50  # Define the number of training epochs\n",
        "\n",
        "model = get_transfer_model('resnet50', num_classes).to('cuda')\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer with learning rate\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5)  # Initialize early stopping with patience\n",
        "\n",
        "train_losses, val_losses = [], []  # Lists to store training and validation losses per epoch\n",
        "\n",
        "# Initialize GradScaler for AMP\n",
        "scaler = GradScaler()\n",
        "# Loop over epochs\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    # Training phase\n",
        "    model.train()  # Set model to training mode\n",
        "    train_loss = 0  # Initialize cumulative training loss for the epoch\n",
        "    # Loop over training data with tqdm progress bar\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "        with autocast():  # Use AMP for mixed-precision calculations\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "        # Scale loss to avoid underflow for float16\n",
        "        scaler.scale(loss).backward()  # Backward pass with scaled loss\n",
        "        scaler.step(optimizer)  # Optimizer step\n",
        "        scaler.update()  # Update the scaler for next iteration\n",
        "        train_loss += loss.item()  # Accumulate the training loss\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0  # Initialize cumulative validation loss for the epoch\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        # Loop over validation data with tqdm progress bar\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            images, labels = images.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "            with autocast():  # Use AMP for mixed-precision calculations\n",
        "                outputs = model(images)  # Forward pass\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "            val_loss += loss.item()  # Accumulate the validation loss\n",
        "\n",
        "    # Calculate average losses and append to lists\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping.check_early_stop(avg_val_loss):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'transfer_learning_resnet50.pth')\n",
        "print(\"Model saved as 'transfer_learning_resnet50.pth'\")"
      ],
      "metadata": {
        "id": "dUbQOLLqiNFP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:15:47.974898Z",
          "iopub.execute_input": "2024-11-03T20:15:47.975865Z",
          "iopub.status.idle": "2024-11-03T20:18:33.399347Z",
          "shell.execute_reply.started": "2024-11-03T20:15:47.975822Z",
          "shell.execute_reply": "2024-11-03T20:18:33.398391Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Curve for ResNet50"
      ],
      "metadata": {
        "id": "G0qaEq3wgoXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting loss curves\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss for ResNet-50\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lItANfDpgQfX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:18:44.165242Z",
          "iopub.execute_input": "2024-11-03T20:18:44.165674Z",
          "iopub.status.idle": "2024-11-03T20:18:44.469779Z",
          "shell.execute_reply.started": "2024-11-03T20:18:44.165634Z",
          "shell.execute_reply": "2024-11-03T20:18:44.468827Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Metrics Calculation"
      ],
      "metadata": {
        "id": "Oxw2GrNCfwY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Move model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():  # Disable gradient calculation for faster inference\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the same device as the model (GPU)\n",
        "        images, labels = images.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Get predictions (choose the class with the highest logit score)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Append predictions and true labels to lists\n",
        "        all_preds.extend(predicted.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "        all_labels.extend(labels.cpu().numpy())    # Move to CPU and convert to numpy\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "U6YzEAWZfxYv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:20:37.089632Z",
          "iopub.execute_input": "2024-11-03T20:20:37.090352Z",
          "iopub.status.idle": "2024-11-03T20:20:39.326020Z",
          "shell.execute_reply.started": "2024-11-03T20:20:37.090309Z",
          "shell.execute_reply": "2024-11-03T20:20:39.324984Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XAI - Grad-CAM, Grad-CAM++, Eigen-CAM"
      ],
      "metadata": {
        "id": "z1LnxOxPfz4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Grad-CAM (Gradient-weighted Class Activation Mapping)**\n",
        "Grad-CAM uses gradients flowing back from a target class to the last convolutional layer of a model to generate a heatmap. This heatmap highlights the areas in the image that are most relevant to the model's classification for a particular class.\n",
        "\n",
        "* **How It Works:** Grad-CAM computes the gradients of the target class score concerning the feature maps of a convolutional layer. These gradients are globally pooled to get a single weight for each feature map. Then, each feature map is multiplied by its corresponding weight, and all feature maps are summed up. Finally, ReLU is applied to keep only the positive influences.\n",
        "* **Interpretation:** The resulting heatmap shows which regions contribute positively to the model's confidence for the specified class. This approach is class-discriminative, meaning it highlights areas of the image that correspond specifically to the predicted class.\n",
        "\n",
        "**2. Grad-CAM++**\n",
        "Grad-CAM++ is an extension of Grad-CAM that refines the heatmap to provide more detailed localization, especially in cases where multiple objects are present or when the object of interest is not dominant in the image.\n",
        "\n",
        "* **How It Works:** Grad-CAM++ introduces additional weighting terms to account for each pixel's importance in the feature maps. It calculates pixel-wise weights rather than global weights for each feature map. These pixel-wise weights are derived from higher-order gradients, allowing Grad-CAM++ to better capture complex and subtle details in an image.\n",
        "* **Interpretation:** Grad-CAM++ often produces sharper, more localized heatmaps, which can be especially useful in medical imaging, fine-grained classification, or images with multiple objects of interest.\n",
        "\n",
        "**3. Eigen-CAM**\n",
        "Eigen-CAM is a technique based on Principal Component Analysis (PCA) of the feature maps, rather than using gradients, to produce a heatmap. It’s an unsupervised approach that doesn’t depend on any specific target class and is class-agnostic.\n",
        "\n",
        "* **How It Works:** Eigen-CAM applies PCA to the feature maps from a chosen convolutional layer. It uses the principal eigenvector of these feature maps to identify regions of high variance, which often correlate with regions that the model finds significant for any class. The resulting heatmap highlights the parts of the image that contribute most to the model’s general activations, rather than to a specific class.\n",
        "* **Interpretation:** Since it’s class-agnostic, Eigen-CAM is ideal for understanding the general features in an image that activate a model, regardless of class. This is useful for cases where we want to understand model behavior without biasing it toward a particular prediction.\n"
      ],
      "metadata": {
        "id": "Wlbb3sxzOO8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned ResNet-50 model\n",
        "num_classes = len(dataset.classes)\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model.load_state_dict(torch.load(\"transfer_learning_resnet50.pth\"))\n",
        "model = model.to('cuda')\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Select a sample image from the test dataset\n",
        "sample_image, _ = test_dataset[0]\n",
        "sample_image = sample_image.unsqueeze(0).to('cuda')  # Add batch dimension and move to GPU\n",
        "\n",
        "# Convert sample image to numpy for visualization\n",
        "original_image_np = sample_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "original_image_np = (original_image_np * 0.5) + 0.5  # Unnormalize\n",
        "original_image_np = np.clip(original_image_np, 0, 1)\n",
        "\n",
        "# Set up Grad-CAM, Grad-CAM++, and Eigen-CAM\n",
        "target_layers = [model.layer4[-1]]\n",
        "\n",
        "# Initialize CAM methods\n",
        "gradcam = GradCAM(model=model, target_layers=target_layers)\n",
        "gradcam_plus_plus = GradCAMPlusPlus(model=model, target_layers=target_layers)\n",
        "eigen_cam = EigenCAM(model=model, target_layers=target_layers)\n",
        "\n",
        "# Run inference to get the predicted class\n",
        "with torch.no_grad():\n",
        "    outputs = model(sample_image)\n",
        "    predicted_class = outputs.argmax().item()\n",
        "    predicted_class_name = class_names[predicted_class]  # Get the class name\n",
        "\n",
        "# Define the target class for CAM methods\n",
        "target = [ClassifierOutputTarget(predicted_class)]\n",
        "\n",
        "# Generate heatmaps using Grad-CAM, Grad-CAM++, and Eigen-CAM\n",
        "gradcam_heatmap = gradcam(input_tensor=sample_image, targets=target)[0]\n",
        "gradcam_pp_heatmap = gradcam_plus_plus(input_tensor=sample_image, targets=target)[0]\n",
        "eigen_cam_heatmap = eigen_cam(input_tensor=sample_image, targets=target)[0]\n",
        "\n",
        "# Overlay the heatmaps on the original image\n",
        "gradcam_result = show_cam_on_image(original_image_np, gradcam_heatmap, use_rgb=True)\n",
        "gradcam_pp_result = show_cam_on_image(original_image_np, gradcam_pp_heatmap, use_rgb=True)\n",
        "eigen_cam_result = show_cam_on_image(original_image_np, eigen_cam_heatmap, use_rgb=True)\n"
      ],
      "metadata": {
        "id": "b8Ddy03lf5bi",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:41:47.990439Z",
          "iopub.execute_input": "2024-11-03T20:41:47.991333Z",
          "iopub.status.idle": "2024-11-03T20:41:48.908248Z",
          "shell.execute_reply.started": "2024-11-03T20:41:47.991290Z",
          "shell.execute_reply": "2024-11-03T20:41:48.906785Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(original_image_np)\n",
        "plt.title(f\"Original Image\\n(Predicted: {predicted_class_name})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(gradcam_result)\n",
        "plt.title(f\"Grad-CAM\\n(Predicted: {predicted_class_name})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(gradcam_pp_result)\n",
        "plt.title(f\"Grad-CAM++\\n(Predicted: {predicted_class_name})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(eigen_cam_result)\n",
        "plt.title(f\"Eigen-CAM\\n(Predicted: {predicted_class_name})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:41:49.642587Z",
          "iopub.execute_input": "2024-11-03T20:41:49.643523Z",
          "iopub.status.idle": "2024-11-03T20:41:50.255970Z",
          "shell.execute_reply.started": "2024-11-03T20:41:49.643454Z",
          "shell.execute_reply": "2024-11-03T20:41:50.255054Z"
        },
        "id": "NMdIb7mNOO8G"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME"
      ],
      "metadata": {
        "id": "moJGe_-fOO8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** LIME explains a model’s prediction for a specific image by highlighting important regions that influenced the prediction.\n",
        "\n",
        "**How Users Can Understand the Classification Output with LIME:**\n",
        "\n",
        "**Highlighted Regions:** LIME’s heatmap shows which parts of the image were most important to the model’s decision. Brighter or colored areas on the heatmap indicate regions that strongly support the model's prediction.\n",
        "**Interpretation:** By viewing these highlighted regions, users can understand what the model \"focused on\" to make its prediction. For example, if a model predicts \"dog\" and LIME highlights the face and body, it indicates these parts were most relevant to the model’s decision."
      ],
      "metadata": {
        "id": "2WyJtJ6YOO8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Load the fine-tuned ResNet-50 model\n",
        "num_classes = len(dataset.classes)\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model.load_state_dict(torch.load(\"transfer_learning_resnet50.pth\"))\n",
        "model = model.to('cuda')\n",
        "model.eval()\n",
        "\n",
        "# Select a sample image from the dataset and preprocess it for display\n",
        "sample_image, _ = dataset[0]  # Get the first image from the dataset\n",
        "original_image_np = sample_image.permute(1, 2, 0).cpu().numpy()\n",
        "original_image_np = (original_image_np * 0.5) + 0.5  # Unnormalize\n",
        "original_image_np = np.clip(original_image_np, 0, 1)  # Ensure valid range [0, 1]\n",
        "\n",
        "# Define a function to make predictions for LIME\n",
        "def batch_predict(images):\n",
        "    model.eval()\n",
        "    # Convert numpy arrays to PIL images and apply transformations\n",
        "    batch = torch.stack([transform_test(Image.fromarray((image * 255).astype(np.uint8))) for image in images], dim=0).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        logits = model(batch)\n",
        "    return logits.cpu().numpy()\n",
        "\n",
        "# Initialize LIME explainer\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Generate LIME explanation for the sample image\n",
        "lime_explanation = explainer.explain_instance(\n",
        "    original_image_np,  # Original image in numpy format\n",
        "    batch_predict,      # Prediction function\n",
        "    top_labels=1,       # Focus on the top predicted class\n",
        "    hide_color=0,\n",
        "    num_samples=100    # Number of perturbed samples\n",
        ")\n",
        "\n",
        "# Get the image and mask for the predicted class\n",
        "predicted_class = model(sample_image.unsqueeze(0).to('cuda')).argmax().item()\n",
        "lime_image, lime_mask = lime_explanation.get_image_and_mask(\n",
        "    label=predicted_class,\n",
        "    positive_only=True,\n",
        "    hide_rest=False,\n",
        "    num_features=10,\n",
        "    min_weight=0.01\n",
        ")\n",
        "lime_image = mark_boundaries(lime_image, lime_mask)\n",
        "\n",
        "# Display the original and LIME result\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(original_image_np)\n",
        "plt.title(f\"Original Image\\n(Predicted: {class_names[predicted_class]})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(lime_image)\n",
        "plt.title(f\"LIME\\n(Predicted: {class_names[predicted_class]})\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-03T20:46:30.757772Z",
          "iopub.execute_input": "2024-11-03T20:46:30.758760Z",
          "iopub.status.idle": "2024-11-03T20:46:33.296991Z",
          "shell.execute_reply.started": "2024-11-03T20:46:30.758706Z",
          "shell.execute_reply": "2024-11-03T20:46:33.296051Z"
        },
        "id": "VCMbW4tsOO8H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "HoURk8cgOO8H"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}